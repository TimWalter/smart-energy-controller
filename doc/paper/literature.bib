% This file was created with Citavi 6.17.0.0

@proceedings{.2021,
 year = {2021},
 title = {2021 40th Chinese Control Conference (CCC)}
}


@misc{.29.12.2023,
 year = {29.12.2023},
 title = {Electricity Maps | Reduce carbon emissions with actionable electricity data},
 url = {https://www.electricitymaps.com/},
 urldate = {29.12.2023}
}


@article{AntoninRaffin.2021,
 author = {{Antonin Raffin} and {Ashley Hill} and {Adam Gleave} and {Anssi Kanervisto} and {Maximilian Ernestus} and {Noah Dormann}},
 year = {2021},
 title = {Stable-Baselines3: Reliable Reinforcement Learning Implementations},
 url = {http://jmlr.org/papers/v22/20-1364.html},
 pages = {1--8},
 volume = {22},
 number = {268},
 journal = {Journal of Machine Learning Research}
}


@article{Basantes.2023,
 author = {Basantes, Jonathan Andr{\'e}s and Paredes, Daniela Estefan{\'i}a and Llanos, Jacqueline Rosario and Ortiz, Diego Edmundo and Burgos, Claudio Danilo},
 year = {2023},
 title = {Energy Management System (EMS) Based on Model Predictive Control (MPC) for an Isolated DC Microgrid},
 pages = {2912},
 volume = {16},
 number = {6},
 journal = {Energies},
 doi = {10.3390/en16062912},
 file = {Basantes, Paredes et al. 2023 - Energy Management System EMS Based:Attachments/Basantes, Paredes et al. 2023 - Energy Management System EMS Based.pdf:application/pdf}
}


@article{Blum.2021,
 author = {Blum, David and Arroyo, Javier and Huang, Sen and Drgo{\v{n}}a, J{\'a}n and Jorissen, Filip and Walnum, Harald Taxt and Chen, Yan and Benne, Kyle and Vrabie, Draguna and Wetter, Michael and Helsen, Lieve},
 year = {2021},
 title = {Building optimization testing framework (BOPTEST) for simulation-based benchmarking of control strategies in buildings},
 pages = {586--610},
 volume = {14},
 number = {5},
 issn = {1940-1493},
 journal = {Journal of Building Performance Simulation},
 doi = {10.1080/19401493.2021.1986574},
 file = {Blum, Arroyo et al. 2021 - Building optimization testing framework BOPTEST:Attachments/Blum, Arroyo et al. 2021 - Building optimization testing framework BOPTEST.pdf:application/pdf}
}


@article{Boyson.2007,
 author = {Boyson, William Earl and Galbraith, Gary M. and King, David L. and Gonzalez, Sigifredo},
 year = {2007},
 title = {Performance model for grid-connected photovoltaic inverters},
 url = {https://www.osti.gov/biblio/920449, journal =},
 doi = {10.2172/920449}
}


@misc{Castellanos.04.12.2022,
 abstract = {As multi-microgrids become readily available, some limited models have been proposed that study operational and power quality constraints with local energy markets independently. This paper proposes a convex optimization model of an energy management system with operational and power quality constraints and interactions in a Local Energy Market (LEM) for unbalanced microgrids (MGs). The LEM consists of a pre-dispatch step and an energy transactions step (ETS). The ETS combines the MGs' objectives while considering two strategies: minimize the cost of buyers or maximize the revenue of sellers. Our proposed model considers harmonic distortion and voltage limit power quality constraints in both steps. Moreover, we model operational constraints such as power flow, power balance, and distributed energy resources behaviors and capacities. We numerically evaluate the proposed model using three unbalanced MGs with residential, industrial, and commercial load profiles, where each microgrid manages its resources locally. Furthermore, we create two groups of cases to analyze the interactions in the local energy market. In the first group, the price of the DSO energy and the surplus from MGs to DSO are the same. The numerical results show that using the increasing revenue strategy promotes MGs to interact more while encouraging them to have high energy prices. When the reducing cost strategy is used, fewer energy interactions occur, and the price of MGs energy is encouraged to be lower.},
 author = {Castellanos, Johanna and Correa-Florez, Carlos Adrian and Garc{\'e}s, Alejandro and Ord{\'o}{\~n}ez-Plata, Gabriel and Uribe, C{\'e}sar A. and Patino, Diego},
 date = {04.12.2022},
 title = {An energy management system model with power quality constraints for  unbalanced multi-microgrids interacting in a local energy market},
 url = {http://arxiv.org/pdf/2212.01910.pdf},
 file = {Castellanos, Correa-Florez et al. 04.12.2022 - An energy management system model:Attachments/Castellanos, Correa-Florez et al. 04.12.2022 - An energy management system model.pdf:application/pdf}
}


@article{Dobos.2012,
 author = {Dobos, Aron P.},
 year = {2012},
 title = {An Improved Coefficient Calculator for the California Energy Commission 6 Parameter Photovoltaic Module Model},
 volume = {134},
 number = {2},
 issn = {0199-6231},
 journal = {Journal of Solar Energy Engineering},
 doi = {10.1115/1.4005759}
}


@article{Ecoffet.2021,
 abstract = {Reinforcement learning promises to solve complex sequential-decision problems autonomously by specifying a high-level reward function only. However, reinforcement learning algorithms struggle when, as is often the case, simple and intuitive rewards provide sparse1 and deceptive2 feedback. Avoiding these pitfalls requires a thorough exploration of the environment, but creating algorithms that can do~so remains one of the central challenges of the field. Here we hypothesize that the main impediment to effective exploration originates from algorithms forgetting how to reach previously visited states (detachment) and failing to first return to a state before exploring from it (derailment). We introduce Go-Explore, a family of algorithms that addresses these two challenges directly through the simple principles of explicitly 'remembering' promising states and returning to such states before intentionally exploring. Go-Explore solves all previously unsolved Atari games and surpasses the state of the art on all hard-exploration games1, with orders-of-magnitude improvements on the grand challenges of Montezuma's Revenge and Pitfall. We also demonstrate the practical potential of Go-Explore on a sparse-reward pick-and-place robotics task. Additionally, we show that adding a goal-conditioned policy can further improve Go-Explore's exploration efficiency and enable it to handle stochasticity throughout training. The substantial performance gains from Go-Explore suggest that the simple principles of remembering states, returning to them, and exploring from them are a powerful and general approach to exploration-an insight that may prove critical to the creation of truly intelligent learning agents.},
 author = {Ecoffet, Adrien and Huizinga, Joost and Lehman, Joel and Stanley, Kenneth O. and Clune, Jeff},
 year = {2021},
 title = {First return, then explore},
 pages = {580--586},
 volume = {590},
 number = {7847},
 journal = {Nature},
 doi = {10.1038/s41586-020-03157-9},
 file = {1901.10995:Attachments/1901.10995.pdf:application/pdf}
}


@article{F.Holmgren.2018,
 author = {{F. Holmgren}, William and {W. Hansen}, Clifford and {A. Mikofski}, Mark},
 year = {2018},
 title = {pvlib python: a python package for modeling solar energy systems},
 pages = {884},
 volume = {3},
 number = {29},
 journal = {Journal of Open Source Software},
 doi = {10.21105/joss.00884},
 file = {F. Holmgren, W. Hansen et al. 2018 - pvlib python:Attachments/F. Holmgren, W. Hansen et al. 2018 - pvlib python.pdf:application/pdf}
}


@misc{GeorgesHebrail.2006,
 author = {{Georges Hebrail}, Alice Berard},
 date = {2006},
 title = {Individual household electric power consumption},
 publisher = {{UCI Machine Learning Repository}},
 doi = {10.24432/C58K54}
}


@misc{Haarnoja.04.01.2018,
 abstract = {Model-free deep reinforcement learning (RL) algorithms have been demonstrated on a range of challenging decision making and control tasks. However, these methods typically suffer from two major challenges: very high sample complexity and brittle convergence properties, which necessitate meticulous hyperparameter tuning. Both of these challenges severely limit the applicability of such methods to complex, real-world domains. In this paper, we propose soft actor-critic, an off-policy actor-critic deep RL algorithm based on the maximum entropy reinforcement learning framework. In this framework, the actor aims to maximize expected reward while also maximizing entropy. That is, to succeed at the task while acting as randomly as possible. Prior deep RL methods based on this framework have been formulated as Q-learning methods. By combining off-policy updates with a stable stochastic actor-critic formulation, our method achieves state-of-the-art performance on a range of continuous control benchmark tasks, outperforming prior on-policy and off-policy methods. Furthermore, we demonstrate that, in contrast to other off-policy algorithms, our approach is very stable, achieving very similar performance across different random seeds.},
 author = {Haarnoja, Tuomas and Zhou, Aurick and Abbeel, Pieter and Levine, Sergey},
 date = {04.01.2018},
 title = {Soft Actor-Critic: Off-Policy Maximum Entropy Deep Reinforcement  Learning with a Stochastic Actor},
 url = {http://arxiv.org/pdf/1801.01290.pdf},
 file = {Haarnoja, Zhou et al. 04.01.2018 - Soft Actor-Critic:Attachments/Haarnoja, Zhou et al. 04.01.2018 - Soft Actor-Critic.pdf:application/pdf}
}


@article{Horsch.2018,
 abstract = {PyPSA-Eur, the first open model dataset of the European power system at the transmission network level to cover the full ENTSO-E area, is presented. It contains 6001 lines (alternating current lines at and above 220 kV voltage level and all high voltage direct current lines), 3657 substations, a new open database of conventional power plants, time series for electrical demand and variable renewable generator availability, and geographic potentials for the expansion of wind and solar power. The model is suitable both for operational studies and generation and transmission expansion planning studies. The continental scope and highly resolved spatial scale enables a proper description of the long-range smoothing effects for renewable power generation and their varying resource availability. The restriction to freely available and open data encourages the open exchange of model data developments and eases the comparison of model results. A further novelty of the dataset is the publication of the full, automated software pipeline to assemble the load-flow-ready model from the original datasets, which enables easy replacement and improvement of the individual parts. This paper focuses on the description of the network topology, the compilation of a European power plant database and a top-down load time-series regionalisation. It summarises the derivation of renewable wind and solar availability time-series from re-analysis weather datasets and the estimation of renewable capacity potentials restricted by land-use. Finally, validations of the dataset are presented, including a new methodology to compare geo-referenced network datasets to one another.},
 author = {H{\"o}rsch, Jonas and Hofmann, Fabian and Schlachtberger, David and Brown, Tom},
 year = {2018},
 title = {PyPSA-Eur: An open optimisation model of the European transmission system},
 url = {http://arxiv.org/pdf/1806.01613.pdf},
 pages = {207--215},
 volume = {22},
 issn = {2211467X},
 journal = {Energy Strategy Reviews},
 doi = {10.1016/j.esr.2018.08.012},
 file = {H{\"o}rsch, Hofmann et al. 2018 - PyPSA-Eur An open optimisation model:Attachments/H{\"o}rsch, Hofmann et al. 2018 - PyPSA-Eur An open optimisation model.pdf:application/pdf}
}


@article{JavierArroyo.2022,
 abstract = {Buildings need advanced control for the efficient and climate-neutral use of their energy systems. Model predictive control (MPC) and reinforcement learning (RL) arise as two powerful control techniques that have been extensively investigated in the literature for their application to building energy management. These methods show complementary qualities in terms of constraint satisfaction, computational demand, adaptability, and intelligibility, but usually a choice is made between both approaches. This paper compares both control approaches and proposes a novel algorithm called reinforced predictive control (RL-MPC) that merges their relative merits. First, the complementarity between RL and MPC is emphasized on a conceptual level by commenting on the main aspects of each method. Second, the RL-MPC algorithm is described that effectively combines features from each approach, namely state estimation, dynamic optimization, and learning. Finally, MPC, RL, and RL-MPC are implemented and evaluated in BOPTEST, a standardized simulation framework for the assessment of advanced control algorithms in buildings. The results indicate that pure RL cannot provide constraint satisfaction when using a control formulation equivalent to MPC and the same controller model for learning. The new RL-MPC algorithm can meet constraints and provide similar performance to MPC while enabling continuous learning and the possibility to deal with uncertain environments.},
 author = {{Javier Arroyo} and {Carlo Manna} and {Fred Spiessens} and {Lieve Helsen}},
 year = {2022},
 title = {Reinforced model predictive control (RL-MPC) for building energy management},
 url = {https://www.sciencedirect.com/science/article/pii/S0306261921015932},
 keywords = {BOPTEST;Building automation;Model predictive control;Reinforced model predictive control;Reinforcement learning},
 pages = {118346},
 volume = {309},
 issn = {0306-2619},
 journal = {Applied Energy},
 doi = {10.1016/j.apenergy.2021.118346}
}


@inproceedings{Jin.2021,
 author = {Jin, Lingwu and Chen, Zheng and Li, Jinwei and Ye, Tao},
 title = {Reinforcement Learning Method Based Load shifting strategy with Demand Response},
 pages = {1586--1591},
 booktitle = {2021 40th Chinese Control Conference (CCC)},
 year = {2021},
 doi = {10.23919/CCC52363.2021.9549975}
}


@article{Nakabi.2021,
 abstract = {Sustainable Energy, Grids and Networks, 25 (2021) 100413. doi:10.1016/j.segan.2020.100413},
 author = {Nakabi, Taha Abdelhalim and Toivanen, Pekka},
 year = {2021},
 title = {Deep reinforcement learning for energy management in a microgrid with flexible demand},
 pages = {100413},
 volume = {25},
 issn = {23524677},
 journal = {Sustainable Energy, Grids and Networks},
 doi = {10.1016/j.segan.2020.100413},
 file = {Nakabi, Toivanen 2021 - Deep reinforcement learning for energy:Attachments/Nakabi, Toivanen 2021 - Deep reinforcement learning for energy.pdf:application/pdf}
}


@misc{Schulman.20.07.2017,
 abstract = {We propose a new family of policy gradient methods for reinforcement learning, which alternate between sampling data through interaction with the environment, and optimizing a {\textquotedbl}surrogate{\textquotedbl} objective function using stochastic gradient ascent. Whereas standard policy gradient methods perform one gradient update per data sample, we propose a novel objective function that enables multiple epochs of minibatch updates. The new methods, which we call proximal policy optimization (PPO), have some of the benefits of trust region policy optimization (TRPO), but they are much simpler to implement, more general, and have better sample complexity (empirically). Our experiments test PPO on a collection of benchmark tasks, including simulated robotic locomotion and Atari game playing, and we show that PPO outperforms other online policy gradient methods, and overall strikes a favorable balance between sample complexity, simplicity, and wall-time.},
 author = {Schulman, John and Wolski, Filip and Dhariwal, Prafulla and Radford, Alec and Klimov, Oleg},
 date = {20.07.2017},
 title = {Proximal Policy Optimization Algorithms},
 url = {http://arxiv.org/pdf/1707.06347.pdf},
 file = {Schulman, Wolski et al. 20.07.2017 - Proximal Policy Optimization Algorithms:Attachments/Schulman, Wolski et al. 20.07.2017 - Proximal Policy Optimization Algorithms.pdf:application/pdf}
}


@phdthesis{Sonderegger.1978,
 author = {Sonderegger, R. C.},
 year = {1978},
 title = {Dynamic models of house heating based on equivalent thermal parameters},
 keywords = {Analog Circuits;Computerized Simulation;Construction;Conversion;Dynamic Models;Energy Consumption;Energy Production;Heating Equipment;Residential Areas;Solar Heating;Thermal Diffusion},
 school = {{Princeton University, New Jersey}}
}


@book{Sutton.2018,
 author = {Sutton, Richard S. and Barto, Andrew G.},
 year = {2018},
 title = {Reinforcement Learning: An Introduction},
 url = {http://incompleteideas.net/book/the-book-2nd.html},
 edition = {Second},
 publisher = {{The MIT Press}}
}


@article{ThomasHuld.2012,
 abstract = {The Photovoltaic Geographical Information System (PVGIS) is a web application for the estimation of the performance of photovoltaic (PV) systems in Europe and Africa, which has become widely used by the PV community in Europe. We here present the results of adapting the solar radiation data calculated from satellite data in the Climate Monitoring Satellite Application Facility (CM-SAF) to PVGIS. The CM-SAF solar radiation database is characterized by very low overall bias and shows good accuracy at validation sites. The application to PVGIS brings important improvements relative to the existing solar radiation databases within PVGIS.},
 author = {{Thomas Huld} and {Richard M{\"u}ller} and {Attilio Gambardella}},
 year = {2012},
 title = {A new solar radiation database for estimating PV performance in Europe and Africa},
 url = {https://www.sciencedirect.com/science/article/pii/S0038092X12001119},
 keywords = {PV performance estimate;PVGIS;Solar radiation mapping},
 pages = {1803--1815},
 volume = {86},
 number = {6},
 issn = {0038-092X},
 journal = {Solar Energy},
 doi = {10.1016/j.solener.2012.03.006}
}


@article{ThomasSchreiber.2020,
 abstract = {With the increasing use of volatile renewable energies, the requirements for building automation and control systems (BACS) are increasing. Load shifting within local energy systems stabilizes fluctuations in the grid and can be triggered by price signals. The energy purchase can thus be considered and solved as an optimal control problem. Classical approaches, often based on the optimization of mathematical models, are uneconomical in many cases, due to the high effort involved in the model creation. Algorithms from the field of Reinforcement Learning (RL), on the other hand, have a high potential for the automation of energy system optimization, due to their model-free and data-driven characteristics. However, there is still a lack of studies that examine algorithms for BACS-related applications in a structured way. Therefore, we present a study, investigating the potential of two different RL algorithms for load shifting in a cooling supply system. We combine the benefits of Modelica, a powerful modeling language, with RL algorithms and demonstrate how generalized relationships and control decisions can be learned. The case study is modeled according to a cooling supply system in Berlin, Germany. The two different algorithms (DQN and DDPG) are used to control the operation parameters of a central compression chiller, with respect to a price signal. While real monitoring data are used as exogenous influences, the thermal dynamics of the cooling network are simulated. With the learned policies, flexibility in the network is used which leads on average to weekly cost savings of 14 {\%}, compared to direct load coverage. Our results suggest that, under certain conditions, RL is a suitable alternative to established methods. However, we also acknowledge that there are still research questions to address before RL can be applied in real BACS.},
 author = {{Thomas Schreiber} and {S{\"o}ren Eschweiler} and {Marc Baranski} and {Dirk M{\"u}ller}},
 year = {2020},
 title = {Application of two promising Reinforcement Learning algorithms for load shifting in a cooling supply system},
 url = {https://www.sciencedirect.com/science/article/pii/S0378778820320922},
 keywords = {Building automation;control;Load shifting;Optimal control;Reinforcement learning;Simulation;Thermal systems},
 pages = {110490},
 volume = {229},
 issn = {0378-7788},
 journal = {Energy and Buildings},
 doi = {10.1016/j.enbuild.2020.110490}
}


@misc{Towers.2023,
 abstract = {An API standard for single-agent reinforcement learning environments, with popular reference environments and related utilities (formerly Gym)},
 author = {Towers, Mark and Terry, Jordan K. and Kwiatkowski, Ariel and Balis, John U. and de Cola, Gianluca and Deleu, Tristan and Goul{\~a}o, Manuel and Kallinteris, Andreas and {KG, Arjun} and Krimmel, Markus and Perez-Vicente, Rodrigo and Pierr{\'e}, Andrea and Schulhoff, Sander and Tai, Jun Jet and Shen, Andrew Tan Jin and Younis, Omar G.},
 year = {2023},
 title = {Gymnasium},
 url = {https://zenodo.org/record/8127025},
 urldate = {2023-07-08},
 publisher = {Zenodo},
 doi = {10.5281/zenodo.8127026}
}


@misc{Wang.20.06.2023,
 abstract = {Warm-Start reinforcement learning (RL), aided by a prior policy obtained from offline training, is emerging as a promising RL approach for practical applications. Recent empirical studies have demonstrated that the performance of Warm-Start RL can be improved \textit{quickly} in some cases but become \textit{stagnant} in other cases, especially when the function approximation is used. To this end, the primary objective of this work is to build a fundamental understanding on ``\textit{whether and when online learning can be significantly accelerated by a warm-start policy from offline RL?}''. Specifically, we consider the widely used Actor-Critic (A-C) method with a prior policy. We first quantify the approximation errors in the Actor update and the Critic update, respectively. Next, we cast the Warm-Start A-C algorithm as Newton's method with perturbation, and study the impact of the approximation errors on the finite-time learning performance with inaccurate Actor/Critic updates. Under some general technical conditions, we derive the upper bounds, which shed light on achieving the desired finite-learning performance in the Warm-Start A-C algorithm. In particular, our findings reveal that it is essential to reduce the algorithm bias in online learning.  We also obtain lower bounds on the sub-optimality gap of the Warm-Start A-C algorithm to quantify the impact of the bias and error propagation.},
 author = {Wang, Hang and Lin, Sen and Zhang, Junshan},
 date = {20.06.2023},
 title = {Warm-Start Actor-Critic: From Approximation Error to Sub-optimality Gap},
 url = {http://arxiv.org/pdf/2306.11271.pdf},
 file = {Wang, Lin et al. 20.06.2023 - Warm-Start Actor-Critic:Attachments/Wang, Lin et al. 20.06.2023 - Warm-Start Actor-Critic.pdf:application/pdf}
}


@misc{Zhang.23.10.2020,
 abstract = {Priority dispatching rule (PDR) is widely used for solving real-world Job-shop scheduling problem (JSSP). However, the design of effective PDRs is a tedious task, requiring a myriad of specialized knowledge and often delivering limited performance. In this paper, we propose to automatically learn PDRs via an end-to-end deep reinforcement learning agent. We exploit the disjunctive graph representation of JSSP, and propose a Graph Neural Network based scheme to embed the states encountered during solving. The resulting policy network is size-agnostic, effectively enabling generalization on large-scale instances. Experiments show that the agent can learn high-quality PDRs from scratch with elementary raw features, and demonstrates strong performance against the best existing PDRs. The learned policies also perform well on much larger instances that are unseen in training.},
 author = {Zhang, Cong and Song, Wen and Cao, Zhiguang and Zhang, Jie and Tan, Puay Siew and Xu, Chi},
 date = {23.10.2020},
 title = {Learning to Dispatch for Job Shop Scheduling via Deep Reinforcement  Learning},
 url = {http://arxiv.org/pdf/2010.12367.pdf},
 file = {Zhang, Song et al. 23.10.2020 - Learning to Dispatch for Job:Attachments/Zhang, Song et al. 23.10.2020 - Learning to Dispatch for Job.pdf:application/pdf}
}


@article{Zhu.2022,
 abstract = {Owing to large industrial energy consumption, industrial production has brought a huge burden to the grid in terms of renewable energy access and power supply. Due to the coupling of multiple energy sources and the uncertainty of renewable energy and demand, centralized methods require large calculation and coordination overhead. Thus, this paper proposes a multi-energy management framework achieved by decentralized execution and centralized training for an industrial park. The energy management problem is formulated as a partially-observable Markov decision process, which is intractable by dynamic programming due to the lack of the prior knowledge of the underlying stochastic process. The objective is to minimize long-term energy costs while ensuring the demand of users. To solve this issue and improve the calculation speed, a novel multi-agent deep reinforcement learning algorithm is proposed, which contains the following key points: counterfactual baseline for facilitating contributing agents to learn better policies, soft actor-critic for improving robustness and exploring optimal solutions. A novel reward is designed by Lagrange multiplier method to ensure the capacity constraints of energy storage. In addition, considering that the increase in the number of agents leads to performance degradation due to large observation spaces, an attention mechanism is introduced to enhance the stability of policy and enable agents to focus on important energy-related information, which improves the exploration efficiency of soft actor-critic. Numerical results based on actual data verify the performance of the proposed algorithm with high scalability, indicating that the industrial park can minimize energy costs under different demands.},
 author = {Zhu, Dafeng and Yang, Bo and Liu, Yuxiang and Wang, Zhaojian and Ma, Kai and Guan, Xinping},
 year = {2022},
 title = {Energy management based on multi-agent deep reinforcement learning for a multi-energy industrial park},
 url = {http://arxiv.org/pdf/2202.03771.pdf},
 pages = {118636},
 volume = {311},
 issn = {0306-2619},
 journal = {Applied Energy},
 doi = {10.1016/j.apenergy.2022.118636},
 file = {Zhu, Yang et al. 2022 - Energy management based on multi-agent:Attachments/Zhu, Yang et al. 2022 - Energy management based on multi-agent.pdf:application/pdf}
}


